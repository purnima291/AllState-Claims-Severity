{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color = 'teal'><center> Feature Engineering & Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.sparse import hstack\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In EDA we had found some pairs of categorical feature that have high amount of correlation among them, so it will be better if I  drop them and threshold selected is 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical column names to be dropped\n",
    "cat_drop = ['cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9','cat50','cat71','cat86',\n",
    "           'cat95','cat96','cat98','cat104']\n",
    "train_df.drop(cat_drop,axis=1,inplace=True)\n",
    "test_df.drop(cat_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There are multiple ways to encode categorical feature. I have also gone through some of the [blogs](https://medium.com/kaggle-blog/allstate-claims-severity-competition-2nd-place-winners-interview-alexey-noskov-f4e4ce18fcfc) related to this competition discussion about ways used to encode features. I am not going to use label encoding or lexical encoding or one hot encoding. <br><br>\n",
    "<font color='chocolate'>My strategy is to combine all these 101 features in to one new column. Which means that each of the row in that new column will contain 101 words long text data(here I am considering single letter as a word). <br>\n",
    "   \n",
    "**So, below cell shows how each row will look like in the new column after all 101 columns values are joined using space between them, for any one data point.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "['A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'D', 'B', 'B', 'D', 'D', 'B', 'D', 'C', 'B', 'B', 'A', 'A', 'A', 'A', 'A', 'D', 'B', 'A', 'T', 'B', 'G', 'A', 'A', 'E', 'G', 'J', 'G', 'BU', 'BC', 'C', 'AS', 'S', 'A', 'O', 'LB']\n"
     ]
    }
   ],
   "source": [
    "word = list(train_df.loc[0,'cat1':'cat116'])\n",
    "print(len(word))\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after joining above list with space will give me a document(in terms of NLP) of lenght 101 words. Shown in below cells. \n",
    "This will be done with all the rows, and resultant will be a feature which can be considered as text column by me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A A B A A A A A A A A A A A B A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A A B A D B B D D B D C B B A A A A A D B A T B G A A E G J G BU BC C AS S A O LB\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be discarding all categorical columns as now all those informations are stored in the form of a new column. <br>\n",
    "Now the question is how to vectorize this text feature that we are going to create in upcoming cells?\n",
    "* I can write down my own custom code for it or use sklearn's Countvectorizer with prediefined vocabulary list. \n",
    "* While applying countvectorizer, I can extract unigram, bigram and trigram features from it.\n",
    "* Also I can try tfidf featurization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_feature(row, df):\n",
    "    word_list = np.array(df.loc[row,'cat1':'cat116'])\n",
    "    text = ' '.join(word_list)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 188318/188318 [22:49<00:00, 137.46it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 125546/125546 [09:07<00:00, 229.51it/s]\n"
     ]
    }
   ],
   "source": [
    "train_df['text'] = None\n",
    "test_df['text'] = None\n",
    "for i in tqdm(range(train_df.shape[0])):\n",
    "    train_df.loc[i,'text'] = new_feature(i,train_df)\n",
    "\n",
    "for i in tqdm(range(test_df.shape[0])):\n",
    "    test_df.loc[i,'text'] = new_feature(i, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving this text columns:\n",
    "with open('train_text.pkl','wb') as f:\n",
    "    pickle.dump(train_df['text'],f)\n",
    "    \n",
    "with open('test_text.pkl','wb') as f:\n",
    "    pickle.dump(test_df['text'],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n",
      "       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14',\n",
      "       'loss', 'text'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#Dropping rest of the categorical columns\n",
    "cat = [col for col in train_df.columns if 'cat' in col]\n",
    "train_df.drop(cat,axis=1,inplace=True)\n",
    "test_df.drop(cat,axis=1,inplace=True)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338\n",
      "['A' 'B' 'D' 'C' 'T' 'G' 'E' 'J' 'BU' 'BC' 'AS' 'S' 'O' 'LB' 'L' 'F' 'I'\n",
      " 'K' 'BI' 'CQ' 'AV' 'BM' 'DP' 'H' 'AB' 'DK' 'AF' 'GK' 'CS' 'N' 'AE' 'DJ'\n",
      " 'P' 'Y' 'CK' 'EB' 'AH' 'LO' 'Q' 'M' 'AX' 'IE' 'DW' 'U' 'LY' 'AM' 'GS'\n",
      " 'AI' 'HK' 'EG' 'AK' 'DC' 'MP' 'DS' 'CL' 'LE' 'HQ' 'BS' 'R' 'HJ' 'AP' 'GC'\n",
      " 'BY' 'AD' 'BT' 'HX' 'HL' 'AL' 'AN' 'HG' 'CO' 'MD' 'LF' 'LM' 'CM' 'CB'\n",
      " 'EL' 'AW' 'AJ' 'AY' 'AT' 'KQ' 'W' 'EE' 'AR' 'AC' 'HN' 'LQ' 'AU' 'DX' 'AQ'\n",
      " 'KW' 'CD' 'IT' 'LN' 'CI' 'CW' 'LC' 'DT' 'GX' 'GE' 'BG' 'CP' 'BO' 'HB'\n",
      " 'GI' 'GM' 'CR' 'JR' 'BD' 'HA' 'V' 'BF' 'BK' 'BA' 'AO' 'LJ' 'IH' 'AG' 'HV'\n",
      " 'DM' 'GU' 'HM' 'CY' 'IC' 'EF' 'BJ' 'KD' 'KI' 'DL' 'DA' 'DN' 'X' 'MG' 'LL'\n",
      " 'KN' 'BQ' 'AA' 'LH' 'BP' 'DF' 'EY' 'LW' 'KA' 'EK' 'EO' 'DH' 'CG' 'HC'\n",
      " 'DI' 'BN' 'FB' 'IG' 'FR' 'CF' 'BL' 'EC' 'KR' 'HI' 'BH' 'IU' 'MC' 'JW'\n",
      " 'FH' 'IF' 'CH' 'KL' 'LX' 'EM' 'IL' 'KB' 'IQ' 'JX' 'GN' 'FD' 'ME' 'KC'\n",
      " 'FT' 'CT' 'GL' 'ES' 'JL' 'BX' 'II' 'HP' 'ED' 'CU' 'EN' 'FG' 'MJ' 'KE'\n",
      " 'DD' 'EI' 'FX' 'CJ' 'EA' 'KP' 'EP' 'FC' 'GB' 'JU' 'LV' 'HW' 'LI' 'GT'\n",
      " 'HH' 'KJ' 'CN' 'FE' 'GA' 'FW' 'IY' 'MO' 'JG' 'ID' 'DU' 'FA' 'LA' 'HR'\n",
      " 'GJ' 'GO' 'KT' 'GW' 'MI' 'BE' 'BR' 'GP' 'KM' 'BV' 'IM' 'LD' 'GR' 'HD'\n",
      " 'KX' 'LR' 'ML' 'KU' 'CE' 'IA' 'DE' 'MU' 'CC' 'CX' 'HY' 'CA' 'EH' 'MA'\n",
      " 'GH' 'LK' 'BB' 'CV' 'IN' 'JM' 'JF' 'KK' 'DR' 'LT' 'GF' 'DO' 'KY' 'MK'\n",
      " 'DV' 'MN' 'GY' 'JC' 'MR' 'JE' 'IP' 'KV' 'KH' 'BW' 'MQ' 'HF' 'FL' 'GV'\n",
      " 'DG' 'JB' 'FU' 'EJ' 'MW' 'DY' 'EW' 'FS' 'JQ' 'DQ' 'JJ' 'HT' 'FF' 'JA'\n",
      " 'GD' 'FV' 'EU' 'FJ' 'LG' 'IR' 'GQ' 'MM' 'MF' 'GG' 'KG' 'JD' 'KS' 'JV'\n",
      " 'EV' 'DB' 'ZZ' 'FK' 'JY' 'KF' 'LU' 'IK' 'JP' 'IJ' 'JO' 'JH' 'JN' 'FP'\n",
      " 'MV' 'IB' 'EQ' 'JT' 'MB' 'JK' 'FI' 'MS' 'HE' 'IV' 'IO' 'FM' 'HO' 'MH'\n",
      " 'MT' 'FO' 'JI' 'FQ' 'FN' 'HU' 'IX']\n"
     ]
    }
   ],
   "source": [
    "#Finding unique words in whole new column: https://stackoverflow.com/a/38558245\n",
    "vocab = train_df['text'].str.split(' ', expand= True).stack().unique()\n",
    "print(len(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Vectorizing categorical features using CountVectorizer\n",
    "* Unigram Bag-of-word(Bow)\n",
    "* Bigram BOW\n",
    "* Trigram BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some feature names:  ['A', 'AA', 'AB', 'AC', 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AJ', 'AK', 'AL', 'AM', 'AN', 'AO', 'AP', 'AQ', 'AR', 'AS', 'AT', 'AU', 'AV', 'AW', 'AX', 'AY', 'B', 'BA', 'BB', 'BC', 'BD', 'BE', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BK', 'BL', 'BM', 'BN', 'BO', 'BP', 'BQ', 'BR', 'BS', 'BT', 'BU', 'BV', 'BW', 'BX', 'BY', 'C', 'CA', 'CB', 'CC', 'CD', 'CE', 'CF', 'CG', 'CH', 'CI', 'CJ', 'CK', 'CL', 'CM', 'CN', 'CO', 'CP', 'CQ', 'CR', 'CS', 'CT', 'CU', 'CV', 'CW', 'CX', 'CY', 'D', 'DA', 'DB', 'DC', 'DD', 'DE', 'DF', 'DG', 'DH', 'DI', 'DJ', 'DK', 'DL', 'DM', 'DN', 'DO', 'DP', 'DQ', 'DR', 'DS', 'DT', 'DU']\n",
      "Length of unigram features:  338\n"
     ]
    }
   ],
   "source": [
    "# Creating unigram features\n",
    "vect = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",lowercase=False)\n",
    "cat_vect_train = vect.fit_transform(train_df['text'])\n",
    "cat_vect_test = vect.transform(test_df['text'])\n",
    "print('Some feature names: ', vect.get_feature_names()[:100])\n",
    "print('Length of unigram features: ',len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_train.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_train,f)\n",
    "\n",
    "with open('unigram_test.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some feature names:  ['A A', 'A AA', 'A AB', 'A AC', 'A AD', 'A AE', 'A AF', 'A AG', 'A AH', 'A AI', 'A AJ', 'A AK', 'A AL', 'A AM', 'A AN', 'A AO', 'A AP', 'A AQ', 'A AR', 'A AS', 'A AT', 'A AU', 'A AV', 'A AW', 'A AX', 'A AY', 'A B', 'A BA', 'A BB', 'A BC', 'A BD', 'A BF', 'A BG', 'A BH', 'A BI', 'A BJ', 'A BK', 'A BM', 'A BN', 'A BO', 'A BT', 'A C', 'A CH', 'A CJ', 'A CL', 'A CR', 'A D', 'A E', 'A EB', 'A F', 'A FE', 'A FU', 'A G', 'A GA', 'A GB', 'A GK', 'A H', 'A I', 'A IC', 'A J', 'A JL', 'A JQ', 'A JR', 'A JW', 'A JX', 'A JY', 'A K', 'A KA', 'A KB', 'A KC', 'A KD', 'A KW', 'A L', 'A LF', 'A LN', 'A LO', 'A M', 'A MC', 'A MD', 'A ME', 'A MO', 'A MP', 'A N', 'A O', 'A P', 'A Q', 'A R', 'A S', 'A T', 'A U', 'A V', 'A W', 'A X', 'A Y', 'AA A', 'AA AD', 'AA AE', 'AA AF', 'AA AG', 'AA AH']\n",
      "Length of bigram features:  4361\n"
     ]
    }
   ],
   "source": [
    "# Creating bigram features\n",
    "vect = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",lowercase=False, ngram_range=(2,2))\n",
    "cat_vect_train = vect.fit_transform(train_df['text'])\n",
    "cat_vect_test = vect.transform(test_df['text'])\n",
    "print('Some feature names: ', vect.get_feature_names()[:100])\n",
    "print('Length of bigram features: ',len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_train.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_train,f)\n",
    "\n",
    "with open('bigram_test.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some feature names:  ['A A A', 'A A AA', 'A A AB', 'A A AD', 'A A AE', 'A A AF', 'A A AG', 'A A AH', 'A A AI', 'A A AJ', 'A A AK', 'A A AL', 'A A AM', 'A A AN', 'A A AO', 'A A AP', 'A A AR', 'A A AS', 'A A AT', 'A A AU', 'A A AV', 'A A AW', 'A A AX', 'A A AY', 'A A B', 'A A BA', 'A A BC', 'A A BD', 'A A BF', 'A A BG', 'A A BJ', 'A A BK', 'A A BM', 'A A BN', 'A A BO', 'A A C', 'A A D', 'A A E', 'A A F', 'A A FU', 'A A G', 'A A GA', 'A A GB', 'A A GK', 'A A H', 'A A I', 'A A J', 'A A JL', 'A A JQ', 'A A JR', 'A A JW', 'A A JX', 'A A JY', 'A A K', 'A A KA', 'A A KB', 'A A KD', 'A A L', 'A A LF', 'A A LN', 'A A LO', 'A A M', 'A A MC', 'A A MD', 'A A MO', 'A A MP', 'A A N', 'A A O', 'A A P', 'A A Q', 'A A R', 'A A S', 'A A T', 'A A U', 'A A V', 'A A X', 'A A Y', 'A AA A', 'A AA AD', 'A AA AE', 'A AA AF', 'A AA AG', 'A AA AH', 'A AA AJ', 'A AA AK', 'A AA AM', 'A AA AN', 'A AA AO', 'A AA AS', 'A AA AT', 'A AA AU', 'A AA AV', 'A AA AW', 'A AA AX', 'A AA AY', 'A AA BA', 'A AA BB', 'A AA BC', 'A AA BF', 'A AA BG']\n",
      "Length of trigram features:  37446\n"
     ]
    }
   ],
   "source": [
    "# Creating trigram features\n",
    "vect = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",lowercase=False, ngram_range=(3,3))\n",
    "cat_vect_train = vect.fit_transform(train_df['text'])\n",
    "cat_vect_test = vect.transform(test_df['text'])\n",
    "print('Some feature names: ', vect.get_feature_names()[:100])\n",
    "print('Length of trigram features: ',len(vect.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigram_train.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_train,f)\n",
    "\n",
    "with open('trigram_test.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Vectorizing categorical features using TfidfVectorizer\n",
    "* Unigram Tfidf feature\n",
    "* Bigram Tfidf feature\n",
    "* Trigram Tfidf feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tfidf unigram\n",
    "vect = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",lowercase=False)\n",
    "cat_vect_train = vect.fit_transform(train_df['text'])\n",
    "cat_vect_test = vect.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_train_tfidf.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_train,f)\n",
    "\n",
    "with open('unigram_test_tfidf.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tfidf bigram\n",
    "vect = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",lowercase=False, ngram_range=(2,2))\n",
    "cat_vect_train = vect.fit_transform(train_df['text'])\n",
    "cat_vect_test = vect.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_train_tfidf.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_train,f)\n",
    "\n",
    "with open('bigram_test_tfidf.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_test,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating tfidf trigram\n",
    "vect = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\",lowercase=False, ngram_range=(3,3))\n",
    "cat_vect_train = vect.fit_transform(train_df['text'])\n",
    "cat_vect_test = vect.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigram_train_tfidf.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_train,f)\n",
    "\n",
    "with open('trigram_test_tfidf.pkl','wb') as f:\n",
    "    pickle.dump(cat_vect_test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handling continuous features\n",
    "In EDA we found that certain pairs of continuous varible have higher association among them, so we will drop them and the threshold is 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['cont9','cont12'], axis=1, inplace=True)\n",
    "test_df.drop(['cont9','cont12'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stacking all features \n",
    "I will be stacking differernt sets categorical features with continuous features. And save them for future use in modelling part. My approach to stack features is:\n",
    "1. Unigram BOW features + Continuous features\n",
    "2. Bigram BOW features + Continuous features\n",
    "3. Trigram BOW features + Continuous features\n",
    "4. Bigram BOW features + Trigram features + Continuous features\n",
    "5. Unigram TFIDF features + Continuous features\n",
    "6. Bigram TFIDF features + Continuous features\n",
    "7. Trigram TFIDF features + Continuous features\n",
    "8. Bigram TFIDF features + Trigram features + Continuous features\n",
    "\n",
    "Some of this stacking approach will lead to increase in number of dimensions. So to overcome this, I will be using some dimensionality reduction techniques.\n",
    "\n",
    "I know that all these sets of stacked features seems overwhelming to work with in modelling part. What can be done, then?\n",
    "Most of the time we observed that xgboost or some other ensemble method works better than some alone ML algorithms(I am saying this just from Kaggle's challenge perspective). So to decide which set of features is good to proceed further, I will be applying xgboost on all these sets of features and whichever set will give best results I will be proceeding with that.\n",
    "\n",
    "### 3.1. Unigram BOW features + Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_train.pkl','rb') as f:\n",
    "    unigram_train = pickle.load(f)\n",
    "    \n",
    "with open('unigram_test.pkl','rb') as f:\n",
    "    unigram_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x (188318, 350)\n",
      "Shape of test_x (125546, 350)\n"
     ]
    }
   ],
   "source": [
    "train_x = hstack((unigram_train,train_df.loc[:,'cont1':'cont14']))\n",
    "test_x = hstack((unigram_test,test_df.loc[:,'cont1':'cont14']))\n",
    "print('Shape of train_x', train_x.shape)\n",
    "print('Shape of test_x', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_bow_train.pkl','wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open('unigram_bow_test.pkl','wb') as f:\n",
    "    pickle.dump(test_x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Bigram BOW features + Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_train.pkl','rb') as f:\n",
    "    bigram_train = pickle.load(f)\n",
    "    \n",
    "with open('bigram_test.pkl','rb') as f:\n",
    "    bigram_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x (188318, 4373)\n",
      "Shape of test_x (125546, 4373)\n"
     ]
    }
   ],
   "source": [
    "train_x = hstack((bigram_train,train_df.loc[:,'cont1':'cont14']))\n",
    "test_x = hstack((bigram_test,test_df.loc[:,'cont1':'cont14']))\n",
    "print('Shape of train_x', train_x.shape)\n",
    "print('Shape of test_x', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_bow_train.pkl','wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open('bigram_bow_test.pkl','wb') as f:\n",
    "    pickle.dump(test_x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Trigram BOW features + Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigram_train.pkl','rb') as f:\n",
    "    trigram_train = pickle.load(f)\n",
    "    \n",
    "with open('trigram_test.pkl','rb') as f:\n",
    "    trigram_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x (188318, 37458)\n",
      "Shape of test_x (125546, 37458)\n"
     ]
    }
   ],
   "source": [
    "train_x = hstack((trigram_train,train_df.loc[:,'cont1':'cont14']))\n",
    "test_x = hstack((trigram_test,test_df.loc[:,'cont1':'cont14']))\n",
    "print('Shape of train_x', train_x.shape)\n",
    "print('Shape of test_x', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigram_bow_train.pkl','wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open('trigram_bow_test.pkl','wb') as f:\n",
    "    pickle.dump(test_x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Bigram BOW features + Trigram features + Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x (188318, 41819)\n",
      "Shape of test_x (125546, 41819)\n"
     ]
    }
   ],
   "source": [
    "train_x = hstack((bigram_train,trigram_train,train_df.loc[:,'cont1':'cont14']))\n",
    "test_x = hstack((bigram_test,trigram_test,test_df.loc[:,'cont1':'cont14']))\n",
    "print('Shape of train_x', train_x.shape)\n",
    "print('Shape of test_x', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bi-trigram_bow_train.pkl','wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open('bi-trigram_bow_test.pkl','wb') as f:\n",
    "    pickle.dump(test_x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Unigram TFIDF features + Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_train_tfidf.pkl','rb') as f:\n",
    "    unigram_train = pickle.load(f)\n",
    "    \n",
    "with open('unigram_test_tfidf.pkl','rb') as f:\n",
    "    unigram_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x (188318, 350)\n",
      "Shape of test_x (125546, 350)\n"
     ]
    }
   ],
   "source": [
    "train_x = hstack((unigram_train,train_df.loc[:,'cont1':'cont14']))\n",
    "test_x = hstack((unigram_test,test_df.loc[:,'cont1':'cont14']))\n",
    "print('Shape of train_x', train_x.shape)\n",
    "print('Shape of test_x', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('unigram_tfidf_train.pkl','wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open('unigram_tfidf_test.pkl','wb') as f:\n",
    "    pickle.dump(test_x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Bigram TFIDF features + Continuous features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_train_tfidf.pkl','rb') as f:\n",
    "    bigram_train = pickle.load(f)\n",
    "    \n",
    "with open('bigram_test_tfidf.pkl','rb') as f:\n",
    "    bigram_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x (188318, 4373)\n",
      "Shape of test_x (125546, 4373)\n"
     ]
    }
   ],
   "source": [
    "train_x = hstack((bigram_train,train_df.loc[:,'cont1':'cont14']))\n",
    "test_x = hstack((bigram_test,test_df.loc[:,'cont1':'cont14']))\n",
    "print('Shape of train_x', train_x.shape)\n",
    "print('Shape of test_x', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bigram_tfidf_train.pkl','wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open('bigram_tfidf_test.pkl','wb') as f:\n",
    "    pickle.dump(test_x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7. Trigram TFIDF features + Continuous features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigram_train_tfidf.pkl','rb') as f:\n",
    "    trigram_train = pickle.load(f)\n",
    "    \n",
    "with open('trigram_test_tfidf.pkl','rb') as f:\n",
    "    trigram_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x (188318, 37458)\n",
      "Shape of test_x (125546, 37458)\n"
     ]
    }
   ],
   "source": [
    "train_x = hstack((trigram_train,train_df.loc[:,'cont1':'cont14']))\n",
    "test_x = hstack((trigram_test,test_df.loc[:,'cont1':'cont14']))\n",
    "print('Shape of train_x', train_x.shape)\n",
    "print('Shape of test_x', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trigram_tfidf_train.pkl','wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open('trigram_tfidf_test.pkl','wb') as f:\n",
    "    pickle.dump(test_x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8. Bigram TFIDF features + Trigram features + Continuous features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_x (188318, 41819)\n",
      "Shape of test_x (125546, 41819)\n"
     ]
    }
   ],
   "source": [
    "train_x = hstack((bigram_train,trigram_train,train_df.loc[:,'cont1':'cont14']))\n",
    "test_x = hstack((bigram_test,trigram_test,test_df.loc[:,'cont1':'cont14']))\n",
    "print('Shape of train_x', train_x.shape)\n",
    "print('Shape of test_x', test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bi-trigram_tfidf_train.pkl','wb') as f:\n",
    "    pickle.dump(train_x,f)\n",
    "    \n",
    "with open('bi-trigram_tfidf_test.pkl','wb') as f:\n",
    "    pickle.dump(test_x,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
